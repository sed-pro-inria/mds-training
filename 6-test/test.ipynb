{
 "metadata": {
  "name": "",
  "signature": "sha256:1edb020bed11c4c4768791c7a101a5f567846a4491079fabd7875b4b0e997d9c"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Software testing"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<img src=\"images/captcha.png\">"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "**Part 1: Software & Unit testing**\n",
      "\n",
      "- Software Testing & Methodologies\n",
      "- Unit Testing - we all have done it!\n",
      "- Advantages / Disadvantages of Unit Testing\n",
      "- Unit Testing Frameworks\n",
      "- Core Techniques & Principles\n",
      "\n",
      "**Part 2: Test Driven Development**\n",
      "\n",
      "- Definition & applicability\n",
      "- Advantages and disadvantages\n",
      "- Practice makes perfect!\n",
      "       \n",
      "**Part 3: Working with Legacy Code**\n",
      "- Code anti-patterns\n",
      "- Approaching existing code\n",
      "- Re-factoring: when, where & how"
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Software Testing & Methodologies\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Software testing is the process of evaluation a software item to detect differences between given input and expected output. While there are various types of software testing (e.g. usability, performance, load, stress testing) a couple of them are worth mentioning:\n",
      "\n",
      "- **black box testing** tests the system based on a specific set of requirements and / or functionalities without considering internal design details\n",
      "- **white box testing** or glass box testing is based on knowing internal aspects of the system, it's architecture, code, etc. thus having a deeper understanding of it\n",
      "- **unit testing** entails testing individual components or parts of code, independent of the rest of the system\n",
      "- **integration testing** ensures whether different modules of the system interact with each other according to the specification or not\n",
      "- **functional testing** ignores the internal parts and focuses on different functions as per specifications.\n"
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "What is a test?"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "A test is the **specification of a usage** of your project, your program or a subset of the\n",
      "functions of your program, together **with the expected result**."
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "hello.c"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%%bash\n",
      "cat > hello.c << EOF\n",
      "#include <stdlib.h>\n",
      "#include <stdio.h>\n",
      "\n",
      "int main()\n",
      "{\n",
      "    printf(\"Hell world!\");\n",
      "    return EXIT_SUCCESS;\n",
      "}\n",
      "EOF"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 43
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%%bash\n",
      "gcc -o hello hello.c"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 44
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Software Requirements Specification"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "A **software requirements specification** (SRS) is a description of a software system to be developed, laying out functional and non-functional requirements, and may include a set of use cases that describe interactions the users will have with the software.\n",
      "\n",
      "When `./hello` is run:\n",
      "- the message \u201c`Hello world!`\u201d is printed,\n",
      "- the program returns with a success as status code."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Ideally, this specification should be **automatizable**: a test is then a program that checks\n",
      "that the program or the function behaves consistently with the specification. \n",
      "Such a program is generally a sequence of function calls and **assertion checks** to verify the behavior of these calls.\n",
      "Automatic testing allows the program to be tested on a regular basis, for example every time the program is compiled, such that the developer is warned for **regressions** as early as possible."
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "hello_test.sh"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%%bash\n",
      "cat > hello_test.sh << EOF\n",
      "#!/usr/bin/env bash\n",
      "\n",
      "# - the message \u201cHello world!\u201d is printed\n",
      "if ! ./hello >/dev/null; then\n",
      "   echo TEST FAILS\n",
      "   exit 1\n",
      "fi\n",
      "\n",
      "# - the program returns with a success as status code\n",
      "if [ \"`./hello`\" != \"Hello world!\" ]; then\n",
      "   echo TEST FAILS\n",
      "   exit 1\n",
      "fi\n",
      "\n",
      "echo SUCCESS\n",
      "EOF\n",
      "chmod +x hello_test.sh"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 45
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%%bash\n",
      "./hello_test.sh"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "TEST FAILS\n"
       ]
      }
     ],
     "prompt_number": 46
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Unit Testing\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We use the term **unit testing** to describe the software testing methodology in which individual parts or units of source code are tested independently in an isolated environment to determine if they are fit for usage.\n",
      "\n",
      "You might not know but all of the programmers out there have done some form of unit testing before, either than being a GUI application with different buttons where on a click of a button a certain functionality is invoked or even a command line program which invokes different methods or classes! That was done to show case scenarios and to help verify some use cases.\n",
      "\n",
      "Unit tests are **fast, isolated, repeatable, self-verifying, timely**."
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Reminder XP's practices"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "- All code must have **unit tests**,\n",
      "- All code must **pass all unit tests before it can be released**,\n",
      "- **When a bug is found** *tests are created before the bug is addressed* (a bug is not an error in logic, it is a test that was not written),\n",
      "- **Acceptance tests are run often** and the results are published."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "| Advantages | Drawbacks\n",
      "| - | - |\n",
      "| Acts as a documentation of the system (understanding from code usage & samples) | It doubles the effort / time invested in development.\n",
      "| Reduces the number of bugs since they are discovered early | Does not show the absence of errors since test code might contain bugs\n",
      "| Good unit testing offers a certain degree of confidence | Good, realistic & useful unit tests are hard to write\n",
      "| Makes it easier to refactor or do integration tests | ..."
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Unit Testing Frameworks"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Using a testing framework provides facilities for **logging test reports** in a conventional form such that other tools like the continuous integration platform can survey test progresses.\n",
      "\n",
      "Each language has it own standard framework for unit testing like **CUnit** for C, **CppUnit** for C++, **NUnit** for .Net, **JUnit** for Java, etc., but all of them share the same key principles.\n",
      "There are many testing platforms out there, but all of them have the same core components / ideas:\n",
      "- a way to add / define **unit tests** and **test suites**\n",
      "- a way to **assert** things: assertEquals(expected, actual)\n",
      "- a way to **execute and view** results.\n",
      "\n",
      "In order to better understand the role of a unit testing framework and its core functionalities we will try to implement from scratch one as below."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%%bash\n",
      "cat > test.h << EOF\n",
      "#ifndef __TEST_H_\n",
      "#define __TEST_H_\n",
      "\n",
      "#include <string>\n",
      "#include <vector>\n",
      "#include <stdio.h>\n",
      "\n",
      "typedef void (*TestMethod)();\n",
      "std::vector<TestMethod> methods;\n",
      "std::vector<std::string> method_names;\n",
      "std::vector<std::string> errors;\n",
      "\n",
      "class Function\n",
      "{\n",
      "public:\n",
      "    Function(TestMethod method, std::string name)\n",
      "    {\n",
      "        methods.push_back(method);\n",
      "        method_names.push_back(name);\n",
      "    }\n",
      "};\n",
      "\n",
      "void assertEq(int expected, int actual)\n",
      "{\n",
      "    if (expected != actual)\n",
      "    {\n",
      "        char szError[256] = {0};\n",
      "        sprintf(szError, \"Expected <%d>, but <%d> found!\", expected, actual);\n",
      "        errors.push_back(szError);\n",
      "    }\n",
      "}\n",
      "\n",
      "void RunAllTests()\n",
      "{\n",
      "    for (size_t i = 0; i < methods.size(); i ++)\n",
      "    {\n",
      "        errors.clear();\n",
      "\n",
      "        printf(\"Running %30s\", method_names[i].c_str());\n",
      "        methods[i]();\n",
      "\n",
      "        printf(\"\\t%s\\r\\n\", errors.size() > 0 ? \"ERR\" : \"OK\");\n",
      "\n",
      "        for (size_t j = 0; j < errors.size(); j ++)\n",
      "            printf(\"\\t%s\\r\\n\", errors[j].c_str());\n",
      "    }\n",
      "}\n",
      "\n",
      "#define TEST_METHOD_INTERNAL(METH, NAME) \\\n",
      "    void METH();\\\n",
      "    Function METH ## METHO(METH, NAME);\\\n",
      "    void METH()\n",
      "#define TEST(X) TEST_METHOD_INTERNAL(X, #X)\n",
      "#define RUN_ALL_TESTS RunAllTests\n",
      "\n",
      "#endif\n",
      "EOF"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 47
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%%bash\n",
      "cat > sample.cpp << EOF\n",
      "\n",
      "// production code: sample fibonacci\n",
      "int f(unsigned int x)\n",
      "{\n",
      "    if(x <= 1) return 1;\n",
      "    else return f(x-1) + f(x-2);\n",
      "}\n",
      "\n",
      "#include \"test.h\"\n",
      "\n",
      "TEST(FibOne)\n",
      "{\n",
      "    assertEq(1, f(1));\n",
      "}\n",
      "\n",
      "TEST(FibFive)\n",
      "{\n",
      "    assertEq(8, f(5));\n",
      "}\n",
      "\n",
      "int main()\n",
      "{\n",
      "    RUN_ALL_TESTS();\n",
      "}\n",
      "EOF\n",
      "g++ sample.cpp -o sample\n",
      "./sample"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Running                         FibOne\tOK\r\n",
        "Running                        FibFive\tOK\r\n"
       ]
      }
     ],
     "prompt_number": 48
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Testing C/C++ programs and functions"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We use the **GTest framework** for writting tests: https://code.google.com/p/googletest/. \n",
      "\n",
      "First you need to install the gtest development package:\n",
      "\n",
      "`sudo apt-get install libgtest-dev`\n",
      "\n",
      "Note that this package only install source files. You have to compile the code yourself to create the necessary library files. These source files should be located at /usr/src/gtest. Browse to this folder and use cmake to compile the library:\n",
      "\n",
      "`sudo apt-get install cmake # install cmake`\n",
      "\n",
      "`cd /usr/src/gtest`\n",
      "\n",
      "`sudo cmake CMakeLists.txt`\n",
      "\n",
      "`sudo make`\n",
      "\n",
      "\n",
      "`sudo cp *.a /usr/lib #copy or symlink libgtest.a and libgtest_main.a to your /usr/lib folder`\n",
      "\n",
      "\n",
      "Using a testing framework provides facilities for **logging test reports** in a conventional form such that other tools like the continuous integration plateform can survey test progresses.\n",
      "\n",
      "There are many testing platforms out there, but all of them have the same core components / ideas:\n",
      "- a way to add / define **unit tests** and **test suites**\n",
      "- a way to **assert** things: assertEquals(expected, actual)\n",
      "- a way to **execute and view** results."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%%bash\n",
      "cat > zero.cpp << EOF\n",
      "\n",
      "int zero(void)\n",
      "{\n",
      "    return 0;\n",
      "}\n",
      "EOF"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 59
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The test will check that `zero()` obeys to its specification, i.e. that the function returns 0. \n",
      "This check is done with the assertion `ASSERT_EQ(0, zero());`.\n",
      "In GTest, tests are organized in **test suites**. \n",
      "Each test suite is performed in a certain execution environment whose **initialization and finalization** can be customized using **test fixtures**. This can be achieved implementing the `::testing::Test` base class and implementing the `SetUp` and `TearDown` functions. \n",
      "The test function `test_zero` is installed in this suite with `TEST` macro."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%%bash\n",
      "cat > test_zero.cpp << EOF\n",
      "#include <gtest/gtest.h>\n",
      "#include <stdlib.h>\n",
      "#include \"zero.cpp\"\n",
      "\n",
      "TEST(zero, test_zero)\n",
      "{\n",
      "    ASSERT_EQ(0, zero());\n",
      "}\n",
      "\n",
      "int main(int argc, char **argv)\n",
      "{\n",
      "    testing::InitGoogleTest(&argc, argv);\n",
      "    return RUN_ALL_TESTS();\n",
      "}\n",
      "EOF"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 60
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "- `ASSERT_TRUE(e)` checks that `e` evaluates to true,\n",
      "- `ASSERT_FALSE(e)` checks that `e` evaluates to false: it is equivalent to `ASSERT_TRUE(!e)`,\n",
      "- `ASSERT_EQ(a, b)` is equivalent to `ASSERT_TRUE(a == b)`."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%%bash\n",
      "cat > CMakeLists.txt << EOF\n",
      "cmake_minimum_required(VERSION 2.6)\n",
      " \n",
      "# Locate GTest\n",
      "find_package(GTest REQUIRED)\n",
      "include_directories(${GTEST_INCLUDE_DIRS})\n",
      " \n",
      "# Link runTests with what we want to test and the GTest and pthread library\n",
      "add_executable(runTests test_zero.cpp)\n",
      "target_link_libraries(runTests gtest ${GTEST_LIBRARIES} pthread)\n",
      "EOF\n",
      "\n",
      "cmake CMakeLists.txt\n",
      "make\n",
      "\n",
      "./runTests"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "-- Configuring done\n",
        "-- Generating done\n",
        "-- Build files have been written to: /home/grosca/mds/6-test\n",
        "[100%] Built target runTests\n",
        "[==========] Running 1 test from 1 test case.\n",
        "[----------] Global test environment set-up.\n",
        "[----------] 1 test from zero\n",
        "[ RUN      ] zero.test_zero\n",
        "[       OK ] zero.test_zero (0 ms)\n",
        "[----------] 1 test from zero (1 ms total)\n",
        "\n",
        "[----------] Global test environment tear-down\n",
        "[==========] 1 test from 1 test case ran. (1 ms total)\n",
        "[  PASSED  ] 1 test.\n"
       ]
      }
     ],
     "prompt_number": 62
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Even tested isolately, some functionalities may need a context to run properly.\n",
      "\n",
      "In the following example, we consider a simple library for random number generation.\n",
      "Testing randomized algorithms typically relies on the fact that we can impose reproducibility by fixing the seed of the pseudo random number generator.\n",
      "The seed may be fixed during **suite initialization**."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%%bash\n",
      "cat > random.cpp << EOF\n",
      "#include <stdlib.h>\n",
      "\n",
      "void random_initialize(int seed)\n",
      "{\n",
      "    srandom(seed);\n",
      "}\n",
      "\n",
      "int random_get_int(int range)\n",
      "{\n",
      "    return random() % range;\n",
      "}\n",
      "EOF"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 66
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%%bash\n",
      "cat > test_random.cpp << EOF\n",
      "#include <gtest/gtest.h>\n",
      "#include <stdlib.h>\n",
      "#include \"random.cpp\"\n",
      "\n",
      "class RandomTest : public ::testing::Test\n",
      "{\n",
      "protected:\n",
      "    virtual void SetUp()\n",
      "    {\n",
      "        random_initialize(5);\n",
      "    }\n",
      "\n",
      "    virtual void TearDown()\n",
      "    {\n",
      "    }\n",
      "};\n",
      "\n",
      "TEST_F(RandomTest, test_random_get_int)\n",
      "{\n",
      "    ASSERT_EQ(5, random_get_int(10));\n",
      "}\n",
      "\n",
      "int main(int argc, char **argv)\n",
      "{\n",
      "    testing::InitGoogleTest(&argc, argv);\n",
      "    return RUN_ALL_TESTS();\n",
      "}\n",
      "EOF"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 67
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%%bash\n",
      "cat > CMakeLists.txt << EOF\n",
      "cmake_minimum_required(VERSION 2.6)\n",
      " \n",
      "# Locate GTest\n",
      "find_package(GTest REQUIRED)\n",
      "include_directories(${GTEST_INCLUDE_DIRS})\n",
      " \n",
      "# Link runTests with what we want to test and the GTest and pthread library\n",
      "add_executable(runTests test_random.cpp)\n",
      "target_link_libraries(runTests gtest ${GTEST_LIBRARIES} pthread)\n",
      "EOF\n",
      "\n",
      "cmake CMakeLists.txt\n",
      "make\n",
      "\n",
      "./runTests"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "-- Configuring done\n",
        "-- Generating done\n",
        "-- Build files have been written to: /home/grosca/mds/6-test\n",
        "[100%] Built target runTests\n",
        "[==========] Running 1 test from 1 test case.\n",
        "[----------] Global test environment set-up.\n",
        "[----------] 1 test from RandomTest\n",
        "[ RUN      ] RandomTest.test_random_get_int\n",
        "[       OK ] RandomTest.test_random_get_int (0 ms)\n",
        "[----------] 1 test from RandomTest (0 ms total)\n",
        "\n",
        "[----------] Global test environment tear-down\n",
        "[==========] 1 test from 1 test case ran. (0 ms total)\n",
        "[  PASSED  ] 1 test.\n"
       ]
      }
     ],
     "prompt_number": 70
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Unit testing"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "A unit test is a test that **only checks a single function**.\n",
      "Writing unit tests for every functions **helps to isolate** which function is broken in case of regression.\n",
      "\n",
      "Unit tests should check in particular the behavior of the functions on the singular points of their parameters.\n",
      "\n",
      "Let us consider a function that is supposed to tell if the nth first elements of an array are sorted increasingly:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%%bash\n",
      "cat > sorted.cpp << EOF\n",
      "#include \"stdio.h\"\n",
      "\n",
      "bool check_nth_first_elements_sorted(const int array[], unsigned int n)\n",
      "{\n",
      "    if (n >= 2)\n",
      "    {\n",
      "        int last = array[0];\n",
      "        for (int i = 1; i < n; i ++)\n",
      "        {\n",
      "            int current = array[i];\n",
      "            \n",
      "            if (current < last)\n",
      "            {\n",
      "                return false;\n",
      "            }\n",
      "            last = current;\n",
      "        }\n",
      "    }\n",
      "    return true;\n",
      "}\n",
      "EOF"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 71
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%%bash\n",
      "cat > test_sorted.cpp << EOF\n",
      "#include <gtest/gtest.h>\n",
      "#include <stdlib.h>\n",
      "#include \"sorted.cpp\"\n",
      "\n",
      "TEST(sorted, test_sorted)\n",
      "{\n",
      "    int array[5] = {0, 1, 3, 3, 2};\n",
      "    ASSERT_TRUE(check_nth_first_elements_sorted(array, 0));\n",
      "    ASSERT_TRUE(check_nth_first_elements_sorted(array, 1));\n",
      "    ASSERT_TRUE(check_nth_first_elements_sorted(array, 2));\n",
      "    ASSERT_TRUE(check_nth_first_elements_sorted(array, 3));\n",
      "    ASSERT_TRUE(check_nth_first_elements_sorted(array, 4));\n",
      "}\n",
      "\n",
      "TEST(sorted, test_not_sorted)\n",
      "{\n",
      "    int array[5] = {0, 1, 3, 3, 2};\n",
      "    ASSERT_FALSE(check_nth_first_elements_sorted(array, 5));\n",
      "}\n",
      "\n",
      "int main(int argc, char **argv)\n",
      "{\n",
      "    testing::InitGoogleTest(&argc, argv);\n",
      "    return RUN_ALL_TESTS();\n",
      "}\n",
      "EOF"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 72
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%%bash\n",
      "cat > CMakeLists.txt << EOF\n",
      "cmake_minimum_required(VERSION 2.6)\n",
      " \n",
      "# Locate GTest\n",
      "find_package(GTest REQUIRED)\n",
      "include_directories(${GTEST_INCLUDE_DIRS})\n",
      " \n",
      "# Link runTests with what we want to test and the GTest and pthread library\n",
      "add_executable(runTests test_sorted.cpp)\n",
      "target_link_libraries(runTests gtest ${GTEST_LIBRARIES} pthread)\n",
      "EOF\n",
      "\n",
      "cmake CMakeLists.txt\n",
      "make\n",
      "\n",
      "./runTests"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "-- Configuring done\n",
        "-- Generating done\n",
        "-- Build files have been written to: /home/grosca/mds/6-test\n",
        "Scanning dependencies of target runTests\n",
        "[100%] Building CXX object CMakeFiles/runTests.dir/test_sorted.cpp.o\n",
        "Linking CXX executable runTests\n",
        "[100%] Built target runTests\n",
        "[==========] Running 2 tests from 1 test case.\n",
        "[----------] Global test environment set-up.\n",
        "[----------] 2 tests from sorted\n",
        "[ RUN      ] sorted.test_sorted\n",
        "[       OK ] sorted.test_sorted (0 ms)\n",
        "[ RUN      ] sorted.test_not_sorted\n",
        "[       OK ] sorted.test_not_sorted (0 ms)\n",
        "[----------] 2 tests from sorted (0 ms total)\n",
        "\n",
        "[----------] Global test environment tear-down\n",
        "[==========] 2 tests from 1 test case ran. (0 ms total)\n",
        "[  PASSED  ] 2 tests.\n"
       ]
      }
     ],
     "prompt_number": 73
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Functional tests"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "A functional test is a test that **checks a certain sequence of function calls** that corresponds to a functionality of the project: typically, this is a sequence that can occur during the typical life cycle of the program."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%%bash\n",
      "cat > counter.cpp << EOF\n",
      "\n",
      "int counter;\n",
      "\n",
      "void reset_counter(void)\n",
      "{\n",
      "    counter = 0;\n",
      "}\n",
      "\n",
      "void increment_counter(void)\n",
      "{\n",
      "    counter++;\n",
      "}\n",
      "\n",
      "int get_counter_value(void)\n",
      "{\n",
      "    return counter;\n",
      "}\n",
      "\n",
      "EOF"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 25
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%%bash\n",
      "cat > test_counter.cpp << EOF\n",
      "#include <gtest/gtest.h>\n",
      "#include <stdlib.h>\n",
      "#include \"counter.cpp\"\n",
      "\n",
      "TEST(counter, test_counter)\n",
      "{\n",
      "    reset_counter();\n",
      "    increment_counter();\n",
      "    increment_counter();\n",
      "    increment_counter();\n",
      "    ASSERT_EQ(3, get_counter_value());\n",
      "}\n",
      "\n",
      "int main(int argc, char **argv)\n",
      "{\n",
      "    testing::InitGoogleTest(&argc, argv);\n",
      "    return RUN_ALL_TESTS();\n",
      "}\n",
      "EOF"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 26
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%%bash\n",
      "cat > CMakeLists.txt << EOF\n",
      "cmake_minimum_required(VERSION 2.6)\n",
      " \n",
      "# Locate GTest\n",
      "find_package(GTest REQUIRED)\n",
      "include_directories(${GTEST_INCLUDE_DIRS})\n",
      " \n",
      "# Link runTests with what we want to test and the GTest and pthread library\n",
      "add_executable(runTests test_counter.cpp)\n",
      "target_link_libraries(runTests gtest ${GTEST_LIBRARIES} pthread)\n",
      "EOF\n",
      "\n",
      "cmake CMakeLists.txt\n",
      "make\n",
      "\n",
      "./runTests"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 27
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Code coverage is the **set of execution paths** (i.e. set of occurrences of instructions in the code) that are covered by the tests. Ideally, this set should be equal to the whole program to ensure that the whole program is tested."
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Core Techniques & Principles"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Sometimes when dealing with large code sets dependencies between components might make unit testing difficult to put in practice. To facilitate unit testing (e.g. remove dependencies) we might want to refactor the production code\n",
      " to allow a certain degree of flexibility. Here are some testing patterns which we might make use of:\n",
      "\n",
      "- **stubs** are objects / components which provides static valid responses\n",
      "- **fakes** are objects / components which implements in a simpler way a functionality (e.g. DAO that uses a Map to store data instead of a real database)\n",
      "- **mocks** objects are used in mock test cases -- they validate that certain methods are called on those objects.\n",
      "\n",
      "Besides being **fast, isolated, repeatable, self-verifying, timely** good unit tests are also:\n",
      "\n",
      "- **trustworthy** each of them tests only one thing, don\u2019t have bugs, and they test the right things.\n",
      "- **maintainable** should have little duplication and test public / protected methods only\n",
      "- **readable** this means not just being able to read a test but also figuring out the problem if the test seems to be wrong. That beings said, tests should be organised per suites, have good names and clean code.\n",
      "\n",
      "To have a better understanding we now present a few anti-patterns:\n",
      "\n",
      "- **constrained test order** when there are certain tests depending on the execution state of others\n",
      "- **hidden test call** when some tests are calling other tests\n",
      "- **(external-)shared-state corruption** when tests are using shared resources (in memory or external)\n"
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Test Driven Development"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "**Test Driven Development** (TDD) is a software development process composed of short development cycles (baby steps technique): first we write a failing test case that defines the improvement / specification then we write the minimum amount of code to make the test pass and finally we refactor the code to acceptable standards.\n",
      "The TDD methodology relies on these three principles:\n",
      "- **test first**, implement after (and every test should initially fail),\n",
      "- **every functionality** should be tested,\n",
      "- the development flows over **short iterations of the cycle: test, code, refactor**.\n",
      "![tdd whell](images/tdd.jpg)"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "In other words this methodology, which has its roots in extreme programming, has the following rules:\n",
      "\n",
      "- Do not write production code unless you write it to make a test pass!\n",
      "- Write the minimum amount of test code to make it fail!\n",
      "- Write the minimum amount of code to make it pass!\n",
      "- Always respect the order: Red -> Green -> Refactor!\n",
      "\n",
      "\n",
      "\n",
      "`TODO: add exercises and links to katas / training sites: FizzBuzz, Stack? or others`\n",
      "\n",
      "\n",
      "`TODO: add trade-offs and applicability`"
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Working with Legacy Code"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "How can one approach existing code through unit testing?"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Whenever a bug was fixed and unit tests which covers that functionality are passing we are now safe to refactor the code. To achieve the best of code refactoring it is strongly recommended to use the **S.O.L.I.D. Principles**:\n",
      "\n",
      "- **Single Responsibility Principle** - a class should have only a single responsibility (i.e. only one potential change in the software's specification should be able to affect the specification of the class)\n",
      "- **Open Closed Principle** - software entities should be open for extension, but closed for modification.\n",
      "- **Liskov Substitution Principle** - objects in a program should be replaceable with instances of their subtypes without altering the correctness of that program.\n",
      "- **Interface Segregation Principle** - many client-specific interfaces are better than one general-purpose interface.\n",
      "- **Dependency Inversion Principles** - one should \u201cDepend upon Abstractions. Do not depend upon concretions..\n"
     ]
    }
   ],
   "metadata": {}
  }
 ]
}
