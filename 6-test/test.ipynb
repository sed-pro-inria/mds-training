{
 "metadata": {
  "name": "",
  "signature": "sha256:37b864ebf5b3c6e6a49d75adf65924ca01fd6dab858ed89fbfb229f052b8a1f4"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Software testing\n",
      "================"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "What is a test?\n",
      "---------------"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "A test is the specification of an usage of your project, your program or a subset of the\n",
      "functions of your program, together with the expected result."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%%bash\n",
      "cat >hello.c <<EOF\n",
      "#include <stdlib.h>\n",
      "#include <stdio.h>\n",
      "\n",
      "int main() {\n",
      "  printf(\"Hello world!\");\n",
      "  return EXIT_SUCCESS;\n",
      "}\n",
      "EOF"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%%bash\n",
      "gcc -o hello hello.c"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Specification : when `./hello` is run,\n",
      "- the message \u201c`Hello world!`\u201d is printed,\n",
      "- the program returns with a success as status code."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Ideally, this specification should be automatizable: a test is then a program that checks\n",
      "that the program or the function behaves consistently with the specification.  Such a program is generally a sequence of function calls and assertion checks to verify the behavior of these calls.\n",
      "Automatic testing allows the program to be tested on a regular basis, for example every time the program is compiled, such that the developer is warned for regressions as early as possible."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%%bash\n",
      "cat >hello_test.sh <<EOF\n",
      "#!bash\n",
      "\n",
      "# - the message \u201c`Hello world!`\u201d is printed\n",
      "if ! ./hello >/dev/null; then\n",
      "   echo TEST FAILS\n",
      "   exit 1\n",
      "fi\n",
      "\n",
      "# - the program returns with a success as status code\n",
      "if [ `./hello` != \"Hello world!\" ]; then\n",
      "   echo TEST FAILS\n",
      "   exit 1\n",
      "fi\n",
      "\n",
      "echo SUCCESS\n",
      "EOF\n",
      "chmod +x hello_test.sh"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%%bash\n",
      "./hello_test.sh"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Testing C programs and functions\n",
      "--------------------------------\n",
      "\n",
      "We use the CUnit framework for writting tests. Using testing framework provides facilities for logging test reports in a conventional form such that other tools like the continuous integration plateform can survey test progresses.\n",
      "Let us illustrate the use of CUnit with a simple function that always returns 0."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%%bash\n",
      "cat >zero.h <<EOF\n",
      "int zero(void);\n",
      "EOF"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 128
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%%bash\n",
      "cat >zero.c <<EOF\n",
      "#include \"zero.h\"\n",
      "\n",
      "int zero(void)\n",
      "{\n",
      "    return 0;\n",
      "}\n",
      "EOF"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 129
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The test will check that `zero()` obeys to its specification, i.e. that the function returns 0. This check is done with the assertion `CU_ASSERT_EQUAL(zero(), 0);`.\n",
      "In CUnit, tests are organized in test suites. Each test suite is performed in a certain execution environment whose initialization and finalization can be customized: here `init_suite()` and `clean_suite()` are kept empty for the sole suite that we declare with `CU_add_suite()`.  The test function `test_zero` is installed in this suite with `CU_add_test()`."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%%bash\n",
      "cat >test_zero.c <<EOF\n",
      "#include <CUnit/Basic.h>\n",
      "#include <stdlib.h>\n",
      "#include \"zero.h\"\n",
      "\n",
      "int init_suite()\n",
      "{\n",
      "    return 0;\n",
      "}\n",
      "\n",
      "int clean_suite()\n",
      "{\n",
      "    return 0;\n",
      "}\n",
      "\n",
      "void test_zero()\n",
      "{\n",
      "    CU_ASSERT_EQUAL(zero(), 0);\n",
      "}\n",
      "\n",
      "void add_tests() {\n",
      "    CU_pSuite suite = CU_add_suite(\"suite\", init_suite, clean_suite);\n",
      "    CU_add_test(suite, \"test_zero: returns 0\", test_zero);\n",
      "}\n",
      "\n",
      "int run_tests() {\n",
      "    unsigned int number_of_tests_failed;\n",
      "    CU_basic_set_mode(CU_BRM_VERBOSE);\n",
      "    CU_basic_run_tests();\n",
      "    number_of_tests_failed = CU_get_number_of_tests_failed();\n",
      "    CU_cleanup_registry();\n",
      "    return number_of_tests_failed;\n",
      "}\n",
      "\n",
      "int main()\n",
      "{\n",
      "    CU_initialize_registry();\n",
      "    add_tests();\n",
      "    if (run_tests() == 0) {\n",
      "         return EXIT_SUCCESS;\n",
      "    }\n",
      "    else {\n",
      "         return EXIT_FAILURE;\n",
      "    }\n",
      "}\n",
      "EOF"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 145
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "- `CU_ASSERT_TRUE(e)` checks that `e` evaluates to true,\n",
      "- `CU_ASSERT(e)` is equivalent to `CU_ASSERT_TRUE(e)`,\n",
      "- `CU_ASSERT_FALSE(e)` checks that `e` evaluates to false: it is equivalent to `CU_ASSERT_TRUE(!e)`,\n",
      "- `CU_ASSERT_EQUAL(a, b)` is equivalent to `CU_ASSERT_TRUE(a == b)`."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%%bash\n",
      "cat >CMakeLists.txt <<EOF\n",
      "cmake_minimum_required(VERSION 2.8)\n",
      "cmake_policy(VERSION 2.8)\n",
      "project(zero)\n",
      "enable_testing()\n",
      "add_executable(test_zero test_zero.c zero.c)\n",
      "target_link_libraries(test_zero cunit)\n",
      "add_test(test_zero test_zero)\n",
      "EOF"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 131
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%%bash\n",
      "mkdir -p build/\n",
      "cd build/\n",
      "cmake ..\n",
      "make"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%%bash\n",
      "cd build\n",
      "ctest"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "CTest can be turned verbose to see the details of which tests of a suite have passed or not.  Verbosity also allows us to see the standard output and error channels of the program, that are otherwise redirected."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%%bash\n",
      "cat >zero.c <<EOF\n",
      "#include \"zero.h\"\n",
      "#include <stdio.h>\n",
      "\n",
      "int zero(void)\n",
      "{\n",
      "    fprintf(stderr, \"zero has been called.\\n\");\n",
      "    return 0;\n",
      "}\n",
      "EOF"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We may check that we don\u2019t see the standard error in default CTest output."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%%bash\n",
      "cd build\n",
      "make\n",
      "ctest"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Scanning dependencies of target test_zero\n",
        "[ 50%] Building C object CMakeFiles/test_zero.dir/test_zero.c.o\n",
        "Linking C executable test_zero\n",
        "[100%] Built target test_zero\n",
        "Test project /home/tmartine/sed/mds-training/test/build\n",
        "    Start 1: test_zero\n",
        "1/1 Test #1: test_zero ........................   Passed    0.00 sec\n",
        "\n",
        "100% tests passed, 0 tests failed out of 1\n",
        "\n",
        "Total Test time (real) =   0.00 sec\n"
       ]
      }
     ],
     "prompt_number": 146
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The option `-V` shows the name of the individual tests that passed or not, and displays the standard channels."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%%bash\n",
      "cd build\n",
      "make\n",
      "ctest -V"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[100%] Built target test_zero\n",
        "UpdateCTestConfiguration  from :/home/tmartine/sed/mds-training/test/build/DartConfiguration.tcl\n",
        "UpdateCTestConfiguration  from :/home/tmartine/sed/mds-training/test/build/DartConfiguration.tcl\n",
        "Test project /home/tmartine/sed/mds-training/test/build\n",
        "Constructing a list of tests\n",
        "Done constructing a list of tests\n",
        "Checking test dependency graph...\n",
        "Checking test dependency graph end\n",
        "test 1\n",
        "    Start 1: test_zero\n",
        "\n",
        "1: Test command: /home/tmartine/sed/mds-training/test/build/test_zero\n",
        "1: Test timeout computed to be: 9.99988e+06\n",
        "1: \n",
        "1: \n",
        "1:      CUnit - A Unit testing framework for C - Version 2.1-0\n",
        "1:      http://cunit.sourceforge.net/\n",
        "1: \n",
        "1: \n",
        "1: Suite: suite\n",
        "1:   Test: test_zero: returns 0 ... passed\n",
        "1: \n",
        "1: --Run Summary: Type      Total     Ran  Passed  Failed\n",
        "1:                suites        1       1     n/a       0\n",
        "1:                tests         1       1       1       0\n",
        "1:                asserts       1       1       1       0\n",
        "1: zero has been called.\n",
        "1/1 Test #1: test_zero ........................   Passed    0.00 sec\n",
        "\n",
        "100% tests passed, 0 tests failed out of 1\n",
        "\n",
        "Total Test time (real) =   0.00 sec\n"
       ]
      }
     ],
     "prompt_number": 147
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Testing context\n",
      "---------------\n",
      "\n",
      "Even tested isolately, some functionalities may need a context to run properly.\n",
      "\n",
      "In the following example, we consider a simple library for random number generation.\n",
      "Testing randomized algorithms typically relies on the fact that we can impose reproducibility by fixing the seed of the pseudo random number generator.\n",
      "The seed may be fixed during suite initialization."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%%bash\n",
      "cat >random.h <<EOF\n",
      "void random_initialize(int seed);\n",
      "int random_get_int(int range);\n",
      "EOF"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 2
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%%bash\n",
      "cat >random.c <<EOF\n",
      "#include \"random.h\"\n",
      "#include <stdlib.h>\n",
      "\n",
      "void random_initialize(int seed)\n",
      "{\n",
      "    srandom(seed);\n",
      "}\n",
      "\n",
      "int random_get_int(int range)\n",
      "{\n",
      "    return random() % range;\n",
      "}\n",
      "EOF"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 3
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%%bash\n",
      "cat >test_random.c <<EOF\n",
      "#include <CUnit/Basic.h>\n",
      "#include <stdlib.h>\n",
      "#include \"random.h\"\n",
      "\n",
      "int init_suite()\n",
      "{\n",
      "    random_initialize(5);\n",
      "    return 0;\n",
      "}\n",
      "\n",
      "int clean_suite()\n",
      "{\n",
      "    return 0;\n",
      "}\n",
      "\n",
      "void test_random_get_int()\n",
      "{\n",
      "    CU_ASSERT_EQUAL(random_get_int(10), 5);\n",
      "}\n",
      "\n",
      "void add_tests() {\n",
      "    CU_pSuite suite = CU_add_suite(\"suite\", init_suite, clean_suite);\n",
      "    CU_add_test(suite, \"test_random_get_int\", test_random_get_int);\n",
      "}\n",
      "\n",
      "int run_tests() {\n",
      "    unsigned int number_of_tests_failed;\n",
      "    CU_basic_set_mode(CU_BRM_VERBOSE);\n",
      "    CU_basic_run_tests();\n",
      "    number_of_tests_failed = CU_get_number_of_tests_failed();\n",
      "    CU_cleanup_registry();\n",
      "    return number_of_tests_failed;\n",
      "}\n",
      "\n",
      "int main()\n",
      "{\n",
      "    CU_initialize_registry();\n",
      "    add_tests();\n",
      "    if (run_tests() == 0) {\n",
      "         return EXIT_SUCCESS;\n",
      "    }\n",
      "    else {\n",
      "         return EXIT_FAILURE;\n",
      "    }\n",
      "}\n",
      "EOF"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 43
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%%bash\n",
      "cat >CMakeLists.txt <<EOF\n",
      "cmake_minimum_required(VERSION 2.8)\n",
      "cmake_policy(VERSION 2.8)\n",
      "project(random)\n",
      "enable_testing()\n",
      "add_executable(test_random test_random.c random.c)\n",
      "target_link_libraries(test_random cunit)\n",
      "add_test(test_random test_random)\n",
      "EOF"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 37
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%%bash\n",
      "mkdir -p build/\n",
      "cd build/\n",
      "cmake ..\n",
      "make"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "-- Configuring done\n",
        "-- Generating done\n",
        "-- Build files have been written to: /home/tmartine/sed/mds-training/test/build\n",
        "Scanning dependencies of target test_random\n",
        "[ 50%] Building C object CMakeFiles/test_random.dir/test_random.c.o\n",
        "Linking C executable test_random\n",
        "[100%] Built target test_random\n"
       ]
      }
     ],
     "prompt_number": 44
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%%bash\n",
      "cd build\n",
      "ctest"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Test project /home/tmartine/sed/mds-training/test/build\n",
        "    Start 1: test_random\n",
        "1/1 Test #1: test_random ......................   Passed    0.00 sec\n",
        "\n",
        "100% tests passed, 0 tests failed out of 1\n",
        "\n",
        "Total Test time (real) =   0.00 sec\n"
       ]
      }
     ],
     "prompt_number": 45
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Unit tests\n",
      "----------"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "A unit test is a test that only checks a single function.\n",
      "Writing unit tests for every functions help to isolate which function is broken in case of regression.\n",
      "Isolating a function often relies on the existence of +mocks+ to simulate the behavior of the other parts of the program.  Mock will be detailed below.\n",
      "\n",
      "Unit tests should check in particular the behavior of the functions on the singular points of their parameters.\n",
      "\n",
      "Let us consider a function that is supposed to tell if the +n+th first elements of an array are sorted increasingly."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%%bash\n",
      "cat >sorted.h <<EOF\n",
      "#include <stdbool.h>\n",
      "bool check_nth_first_elements_sorted(const int array[], unsigned int n);\n",
      "EOF"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 53
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%%bash\n",
      "cat >sorted.c <<EOF\n",
      "#include \"sorted.h\"\n",
      "\n",
      "bool check_nth_first_elements_sorted(const int array[], unsigned int n) {\n",
      "  if (n >= 2) {\n",
      "    int i;\n",
      "    int last = array[0];\n",
      "    for (i = 1; i < n; i++) {\n",
      "      int current = array[i];\n",
      "      if (current < last) {\n",
      "        return false;\n",
      "      }\n",
      "      last = current;\n",
      "    }\n",
      "  }\n",
      "  return true;\n",
      "}\n",
      "EOF"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 118
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The specification of sorted is as follows:\n",
      "- an array with 0 or 1 element is always sorted,\n",
      "- sorted arrays may have several identical cells in a row.\n",
      "\n",
      "In the following test suite, corner cases are tested separately."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%%bash\n",
      "cat >test_sorted.c <<EOF\n",
      "#include <CUnit/Basic.h>\n",
      "#include <stdlib.h>\n",
      "#include \"sorted.h\"\n",
      "\n",
      "int init_suite()\n",
      "{\n",
      "    return 0;\n",
      "}\n",
      "\n",
      "int clean_suite()\n",
      "{\n",
      "    return 0;\n",
      "}\n",
      "\n",
      "int array[6] = {0, 1, 3, 4, 4, 2};\n",
      "\n",
      "void test_sorted_empty_array_or_singleton() {\n",
      "    CU_ASSERT(check_nth_first_elements_sorted(array, 0));\n",
      "    CU_ASSERT(check_nth_first_elements_sorted(array, 1));\n",
      "}\n",
      "\n",
      "void test_sorted_more_than_one_element() {\n",
      "    CU_ASSERT(check_nth_first_elements_sorted(array, 2));\n",
      "}\n",
      "\n",
      "void test_sorted_two_identical_elements_in_a_row() {\n",
      "    CU_ASSERT(check_nth_first_elements_sorted(array, 5));\n",
      "}\n",
      "\n",
      "void test_sorted_unsorted() {\n",
      "    CU_ASSERT_FALSE(check_nth_first_elements_sorted(array, 6));\n",
      "}\n",
      "\n",
      "void add_tests() {\n",
      "    CU_pSuite suite = CU_add_suite(\"suite\", init_suite, clean_suite);\n",
      "    CU_add_test(suite, \"test_sorted: empty array or singleton\", test_sorted_empty_array_or_singleton);\n",
      "    CU_add_test(suite, \"test_sorted: more than one element\", test_sorted_more_than_one_element);\n",
      "    CU_add_test(suite, \"test_sorted: two identical elements in a row\", test_sorted_two_identical_elements_in_a_row);\n",
      "    CU_add_test(suite, \"test_sorted: unsorted array\", test_sorted_unsorted);\n",
      "}\n",
      "\n",
      "int run_tests() {\n",
      "    unsigned int number_of_tests_failed;\n",
      "    CU_basic_set_mode(CU_BRM_VERBOSE);\n",
      "    CU_basic_run_tests();\n",
      "    number_of_tests_failed = CU_get_number_of_tests_failed();\n",
      "    CU_cleanup_registry();\n",
      "    return number_of_tests_failed;\n",
      "}\n",
      "\n",
      "int main()\n",
      "{\n",
      "    CU_initialize_registry();\n",
      "    add_tests();\n",
      "    if (run_tests() == 0) {\n",
      "         return EXIT_SUCCESS;\n",
      "    }\n",
      "    else {\n",
      "         return EXIT_FAILURE;\n",
      "    }\n",
      "}\n",
      "EOF"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 122
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%%bash\n",
      "cat >CMakeLists.txt <<EOF\n",
      "cmake_minimum_required(VERSION 2.8)\n",
      "cmake_policy(VERSION 2.8)\n",
      "project(random)\n",
      "enable_testing()\n",
      "add_executable(test_sorted test_sorted.c sorted.c)\n",
      "target_link_libraries(test_sorted cunit)\n",
      "add_test(test_sorted test_sorted)\n",
      "EOF"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 49
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%%bash\n",
      "mkdir -p build/\n",
      "cd build/\n",
      "cmake ..\n",
      "make"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "-- Configuring done\n",
        "-- Generating done\n",
        "-- Build files have been written to: /home/tmartine/sed/mds-training/test/build\n",
        "Scanning dependencies of target test_sorted\n",
        "[ 50%] Building C object CMakeFiles/test_sorted.dir/test_sorted.c.o\n",
        "Linking C executable test_sorted\n",
        "[100%] Built target test_sorted\n"
       ]
      }
     ],
     "prompt_number": 123
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%%bash\n",
      "cd build\n",
      "ctest"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Test project /home/tmartine/sed/mds-training/test/build\n",
        "    Start 1: test_sorted\n",
        "1/1 Test #1: test_sorted ......................   Passed    0.00 sec\n",
        "\n",
        "100% tests passed, 0 tests failed out of 1\n",
        "\n",
        "Total Test time (real) =   0.00 sec\n"
       ]
      }
     ],
     "prompt_number": 127
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Functional tests\n",
      "----------------"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "A functional test is a test that checks a certain sequence of function calls that corresponds to a functionality of the project: typically, this is a sequence that can occur during the typical life cycle of the program."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%%bash\n",
      "cat >counter.h <<EOF\n",
      "void reset_counter(void);\n",
      "\n",
      "void increment_counter(void);\n",
      "\n",
      "int get_counter_value(void);\n",
      "EOF"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 14
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%%bash\n",
      "cat >counter.c <<EOF\n",
      "#include \"counter.h\"\n",
      "\n",
      "int counter;\n",
      "\n",
      "void reset_counter(void) {\n",
      "    counter = 0;\n",
      "}\n",
      "\n",
      "void increment_counter(void) {\n",
      "    counter++;\n",
      "}\n",
      "\n",
      "int get_counter_value(void) {\n",
      "    return counter;\n",
      "}\n",
      "\n",
      "EOF"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 15
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%%bash\n",
      "cat >test_counter.c <<EOF\n",
      "#include <CUnit/Basic.h>\n",
      "#include <stdlib.h>\n",
      "#include \"counter.h\"\n",
      "\n",
      "int init_suite()\n",
      "{\n",
      "    return 0;\n",
      "}\n",
      "\n",
      "int clean_suite()\n",
      "{\n",
      "    return 0;\n",
      "}\n",
      "\n",
      "void test_counter()\n",
      "{\n",
      "    reset_counter();\n",
      "    increment_counter();\n",
      "    increment_counter();\n",
      "    increment_counter();\n",
      "    CU_ASSERT_EQUAL(get_counter_value(), 3);\n",
      "}\n",
      "\n",
      "void add_tests() {\n",
      "    CU_pSuite suite = CU_add_suite(\"suite\", init_suite, clean_suite);\n",
      "    CU_add_test(suite, \"test_counter\", test_counter);\n",
      "}\n",
      "\n",
      "int run_tests() {\n",
      "    unsigned int number_of_tests_failed;\n",
      "    CU_basic_set_mode(CU_BRM_VERBOSE);\n",
      "    CU_basic_run_tests();\n",
      "    number_of_tests_failed = CU_get_number_of_tests_failed();\n",
      "    CU_cleanup_registry();\n",
      "    return number_of_tests_failed;\n",
      "}\n",
      "\n",
      "int main()\n",
      "{\n",
      "    CU_initialize_registry();\n",
      "    add_tests();\n",
      "    if (run_tests() == 0) {\n",
      "         return EXIT_SUCCESS;\n",
      "    }\n",
      "    else {\n",
      "         return EXIT_FAILURE;\n",
      "    }\n",
      "}\n",
      "EOF"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%%bash\n",
      "cat >CMakeLists.txt <<EOF\n",
      "cmake_minimum_required(VERSION 2.8)\n",
      "cmake_policy(VERSION 2.8)\n",
      "project(counter)\n",
      "enable_testing()\n",
      "add_executable(test_counter test_counter.c counter.c)\n",
      "target_link_libraries(test_counter cunit)\n",
      "add_test(test_counter test_counter)\n",
      "EOF"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%%bash\n",
      "mkdir -p build\n",
      "cd build\n",
      "cmake ..\n",
      "make"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "-- Configuring done\n",
        "-- Generating done\n",
        "-- Build files have been written to: /home/tmartine/sed/mds-training/test/build\n",
        "[100%] Built target test_counter\n"
       ]
      }
     ],
     "prompt_number": 26
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%%bash\n",
      "cd build\n",
      "ctest"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "\n",
        "     CUnit - A Unit testing framework for C - Version 2.1-0\n",
        "     http://cunit.sourceforge.net/\n",
        "\n",
        "\n",
        "Suite: suite\n",
        "  Test: test_counter ... passed\n",
        "\n",
        "--Run Summary: Type      Total     Ran  Passed  Failed\n",
        "               suites        1       1     n/a       0\n",
        "               tests         1       1       1       0\n",
        "               asserts       1       1       1       0\n"
       ]
      }
     ],
     "prompt_number": 21
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Mock\n",
      "----\n",
      "\n",
      "To test independently each program part, it is usual to write +mocks+ that simulate the behavior of one part for testing purpose, in order for other parts to be tested even if that part is broken.\n",
      "\n",
      "Let us suppose that we want to test some random algorithm independently of the actual implementation of the pseudo random number generator.  The `random.c` module becomes an abstraction over the generator (actually, the generator of the standard library).  The normal behavior is to call the generator defined in the library, the +mock+ behavior is to simulate a reduced version of it."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%%bash\n",
      "cat >random.c <<EOF\n",
      "#include \"random.h\"\n",
      "#include <stdlib.h>\n",
      "\n",
      "void random_initialize(int seed)\n",
      "{\n",
      "#ifndef RANDOM_MOCK\n",
      "    srandom(seed);\n",
      "#endif /* RANDOM_MOCK */\n",
      "}\n",
      "\n",
      "int random_get_int(int range)\n",
      "{\n",
      "#ifdef RANDOM_MOCK\n",
      "    return 5;\n",
      "#else\n",
      "    return random() % range;\n",
      "#endif /* RANDOM_MOCK */\n",
      "}\n",
      "EOF"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 177
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%%bash\n",
      "cat >CMakeLists.txt <<EOF\n",
      "cmake_minimum_required(VERSION 2.8)\n",
      "cmake_policy(VERSION 2.8)\n",
      "project(random)\n",
      "add_definitions(-DRANDOM_MOCK)\n",
      "enable_testing()\n",
      "add_executable(test_random test_random.c random.c)\n",
      "target_link_libraries(test_random cunit)\n",
      "add_test(test_random test_random)\n",
      "EOF"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 178
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%%bash\n",
      "mkdir -p build\n",
      "cd build\n",
      "cmake ..\n",
      "make\n",
      "ctest"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "-- Configuring done\n",
        "-- Generating done\n",
        "-- Build files have been written to: /home/tmartine/sed/mds-training/test/build\n",
        "Scanning dependencies of target test_random\n",
        "[ 50%] Building C object CMakeFiles/test_random.dir/random.c.o\n",
        "Linking C executable test_random\n",
        "[100%] Built target test_random\n",
        "Test project /home/tmartine/sed/mds-training/test/build\n",
        "    Start 1: test_random\n",
        "1/1 Test #1: test_random ......................***Failed    0.00 sec\n",
        "\n",
        "0% tests passed, 1 tests failed out of 1\n",
        "\n",
        "Total Test time (real) =   0.00 sec\n",
        "\n",
        "The following tests FAILED:\n",
        "\t  1 - test_random (Failed)\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "Errors while running CTest\n"
       ]
      }
     ],
     "prompt_number": 179
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Code Coverage\n",
      "-------------\n",
      "\n",
      "Code coverage is the set of execution paths (i.e., set of occurrences of instructions in the code) that are covered by the tests.  Ideally, this set should be equal to the whole program to ensure that the whole program is tested."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Macros\n",
      "------\n",
      "\n",
      "`CU_add_test()` and `CU_add_suite()` are functions that may fail if the CUnit environment is not properly initialized.  It is a good practice to check that their result indicates a success, and to signal the error otherwise.  We define a couple of macros to handle that."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#define ADD_TEST_TO_SUITE(suite, test) \\\n",
      "if ((CU_add_test(suite, #test, test) == NULL)) { \\\n",
      "    CU_cleanup_registry(); \\\n",
      "    return CU_get_error(); \\\n",
      "}\n",
      "\n",
      "#define ADD_SUITE_TO_REGISTRY(suite) \\\n",
      "suite = CU_add_suite(#suite, init_suite, clean_suite); \\\n",
      "if (suite == NULL) { \\\n",
      "  CU_cleanup_registry(); \\\n",
      "  return CU_get_error(); \\\n",
      "}"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "ADD_SUITE_TO_REGISTRY(suite);\n",
      "ADD_TEST_TO_SUITE(suite, test_counter);"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Test-Driven Development\n",
      "-----------------------\n",
      "\n",
      "The Test-Driven Development methodology relies on these three principles:\n",
      "- test first, implement after (and every test should initially fail),\n",
      "- every functionality should be tested,\n",
      "- the development flows over short iterations of the cycle: test, code, refactor.\n",
      "![tdd whell](images/tdd.jpg)"
     ]
    }
   ],
   "metadata": {}
  }
 ]
}